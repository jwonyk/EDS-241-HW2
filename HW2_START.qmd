---
title: "üå¨Ô∏èüó≥ Assignment 2: Wind Turbines, Matching, and Difference-in-Differences"
subtitle: "Replicate causal inference identification strategies in Stokes (2015) "
author: "EDS 241 / ESM 244 (DUE: 2/4/26)"
format:
  html:
    theme: sketchy
    css: styles.css
    html-math-method: katex
date: "January 26, 2026"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

### Assignment instructions

Working with classmates to troubleshoot code and concepts is encouraged. If you collaborate, list collaborators at the top of your submission.

All written responses must be written independently (in your own words).

Keep your work readable: Use clear headings and label plot elements thoughtfully.

Assignment submission (YOUR NAME): Jay Kim

------------------------------------------------------------------------

### Introduction

In this assignment you will be doing political weather forecasting except the ‚Äústorms‚Äù we care about are electoral swings that might follow local wind turbine development.

In Stokes (2015), the idea is that a policy with diffuse benefits (cleaner electricity) can create concentrated local costs (turbines nearby), and those local opponents may ‚Äúsend a signal‚Äù at the ballot box (i.e., NIMBYISM). Your job is to use two statistical tools:

-   Matching: Can we create a more apples-to-apples comparison between precincts that did vs. did not end up near turbine proposals?
-   Fixed effects + Difference-in-Differences: Can we use repeated elections to estimate how within-precinct changes in turbine exposure relate to changes in incumbent vote share?

------------------------------------------------------------------------

### Learning goal: Replicate the matching and fixed effects analyses from study:

> Stokes (2015): *"Electoral Backlash against Climate Policy: A Natural Experiment on Retrospective Voting and Local Resistance to Public Policy*.

-   **Study:** [Stokes (2015) - Article](https://drive.google.com/file/d/1y2Okzjq2EA43AW5JzCvFS8ecLpeP6NKh/view?usp=sharing)
-   **Data source:** [Dataverse-Stokes2015](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SDUGCC)

::: callout
`NOTE:` Replication of study estimates will be approximate. An alternative matching procedure and fixed effects estimation package are utilized in this assignment for illustration purposes.
:::

------------------------------------------------------------------------

### Setup: Load libraries

0.  Load libraries (+ install if needed)

```{r}
set.seed(123)
library(tidyverse)
library(here)
library(janitor)
library(jtools)

library(gtsummary)
library(gt)

library(MatchIt) # matching
library(cobalt)  # balance + love plots

library(fixest) # fast fixed effects
library(scales) # plotting

```

------------------------------------------------------------------------

### Part 1: Study Background

#### **1A.** Dive into the details of the study design and evaluation plan

> Goal: Get familiar with the study setting, environmental issue, and policy under evaluation.

::: callout
`NOTE:` Read over study to inform your response to the assignment questions. For this assignment we will skip-over sections that describe the *Instrumental Variables* identification strategy. We will cover instrumental variable designs weeks 6-7.
:::

**1A.Q1** Summarize the environmental policy issue, the outcome of interest, and the intervention being evaluated. Be sure to include a brief description of each of the following key elements of the study: unit of analysis, outcome, treatment, comparison group):

*Response:*

Ontario expanded a climate (renewable-energy policy) that encouraged wind turbine development, often near rural communities. The *unit of analysis* is an election precinct. The *main* *outcome* is the incumbent Liberal Party‚Äôs vote share in provincial elections. The *treatment* is whether a precinct has a wind turbine proposal or an operational turbine (and, in some areas, whether it is within a certain distance, such as 3 km). The *comparison group* consists of precincts in the same wind-suitable districts that did not have a proposal or an operational turbine (or were farther away).

**1A.Q2** Why might turbine proposals be correlated with baseline political preferences or rural areas? Provide 2 plausible mechanisms, and explain why that creates confounding.

*Response:*

1.  Wind resources and land availability - Turbines are more likely to be located where wind is strong and land is available. Rural areas may already have different political preferences than urban areas, so turbines could appear to cause voting differences that were already present.

2.  Local demographics and economic structure - Rural precincts often differ in income, education, and population density. These factors can influence both turbine siting decisions and voting behavior, creating confounding.

------------------------------------------------------------------------

#### **1B.** Break down the causal inference strategy and identify threats to identification:

**1B.Q1** What is the key identifying assumption for a fixed effects / Difference-in-Difference design? Explain how this assumption when satisfied provides evidence of causal effect:

*Response:*

The key assumption is that, in the absence of turbines, treated, and control precincts that would have experienced similar changes in Liberal vote share over time. If this holds, differences in changes can be attributed to turbine exposure rather than pre-existing differences.

**1B.Q2** What is the reason for using a fixed effects approach from a causal inference perspective? Summarize within the context of study (in your own words).

*Response:*

Fixed effects allow each precinct to be compared to itself over time, removing time-invariant differences such as long-standing political culture or geography. This helps isolate whether vote share changes when turbine exposure changes.

**1B.Q3** What part of the SUTVA assumption is most likely violated in the context of this study design (and why)?

*Response:*

The no-spillover (no interference) assumption is most likely violated because turbines can be visible across precinct boundaries, potentially affecting nearby ‚Äúcontrol‚Äù precincts.

**1B.Q4** Why does spillover matter when estimating an unbiased treatment effect?

*Response:*

If control precincts are indirectly affected by nearby turbines, treated and control outcomes become more similar, biasing the estimated treatment effect toward zero.

**1B.Q5** How do the authors assess the risk of spillovers, and what analytic choice do they make to attempt to mitigate the risk that spillover biases the causal estimate?

*Response:*

They explicitly consider distance from turbines; when estimating effects for a given distance band, the authors exclude nearby precincts from the control group to reduce indirect exposure.

------------------------------------------------------------------------

### Part 2: Matching

------------------------------------------------------------------------

We will start by evaluating the 2007 survey (cross-sectional) data. Treatment is defined by whether a precinct is near a turbine proposal (within 3 km).

> Goal: Match precincts using pre-treatment covariates and then estimate the effect of proposed wind turbines on incumbent vote share.

#### **2A.** Load data for matching

1.  Read in data file `stokes15_survey2007.csv`
2.  Code `precinct_id` and `district_id` as factors
3.  Take a look at the data

```{r}
match_data <- read_csv(here("data", "stokes15_survey2007.csv")) %>% 
    mutate(precinct_id = as.factor(precinct_id),
           district_id = as.factor(district_id))
```

**2A.Q1** Intuition check: **Why match?** Explain rationale for using this method.

*Response:*

Matching helps create a control group that resembles treated precincts in pre-treatment characteristics, making comparisons more credible and reducing confounding.

------------------------------------------------------------------------

#### **2B.** Check imbalance (before matching)

-   Create a covariate *balance table* comparing treated and control precincts
-   Treatment indicator: `proposed_turbine_3km`
-   Include pre-treatment covariates: `log_home_val_07`, `p_uni_degree`, `log_median_inc`, `log_pop_denc`
-   Use the `tbl_summary()` function from the `{gtsummary}` package.

```{r}
match_data %>% 
  select(
    proposed_turbine_3km, log_home_val_07, p_uni_degree, log_median_inc,
    log_pop_denc) %>%
  tbl_summary(by = proposed_turbine_3km,
              statistic = list(all_continuous() ~ "{mean} ({sd})",
                               all_categorical() ~ "{n} ({p}%)")) %>%
  modify_header(label ~ "**Covariate**") %>%
  modify_spanning_header(c("stat_1", "stat_2") ~ "**Group**")
```

**2B.Q1** Summarize the table output: Which covariates look balanced/imbalanced?

*Response:*

Population density (`log_pop_denc`) and the percentage with a university degree (`p_uni_degree`) appear clearly imbalanced between the treated and control precincts. A mean difference for `p_uni_degree`, 0.04, when the SD is about 0.10 is large and a mean difference for `log_pop_denc` 1.58 which is huge as it is. Home values (`log_home_val_07`) and median income (`log_median_inc`) look relatively balanced as both mean difference are tiny relative to standard deviation.

**2B.Q2** Describe in your own words why these covariates might be expected to confound the treatment estimate:

*Response (2-4 sentences):*

The main threat to causal inference using fixed-effects estimators is time-varying confounding variables that differ across the treated and control groups. Thus, for the average treatment effect (ATE) to be interpreted causally, the parallel trends assumption must hold. To provide evidence to support the parallel trends assumption, the mean level of the outcome variable by year is plotted below, showing parallel trends pre-treatment. These variables capture socioeconomic and rural‚Äìurban differences that influence both turbine siting and voting behavior. If not controlled for, they could falsely attribute political differences to turbines rather than underlying characteristics.

------------------------------------------------------------------------

**2B.Q3** Intuition check: What type of data do you need to conduct a matching analysis?

*Response:*

Before matching, population density shows the largest imbalance, and median income shows the smallest. After matching, all covariates are well balanced, with median income closest to zero and population density still the largest, but very small.

------------------------------------------------------------------------

### Conduct matching estimation using the {`MatchIt`} package:

üìú [Documentation - MatchIt](https://kosukeimai.github.io/MatchIt/)

Learning goals:

-   Approximate the Mahalanobis matching method used in Stokes (2015)
-   Implement another common matching approach called `propensity score matching`

::: callout
`NOTE`: In the replication code associated with Stokes (2015) the {`AER`} package is used for Mahalanobis matching. In this assignment we use the {`MatchIt`} package. The results are comparable but will not be exactly the same.
:::

------------------------------------------------------------------------

### 2C. Mahalanobis nearest-neighbor matching

-   Conduct Mahalanobis matching\
-   Use nearest-neighbor match without replacement using Mahalanobis distance
-   Use 1-to-1 matching (match one control unit to each treatment unit)
-   Extract the matched data using `match.data()`

```{r}
set.seed(2412026)

match_model <- matchit(
    
    proposed_turbine_3km ~ log_home_val_07 + p_uni_degree + 
        log_median_inc + log_pop_denc,
    data = match_data, 
    method = "nearest",       # Nearest neighbor matching
    distance = "mahalanobis", # Mahalanobis distance
    ratio = 1,                # Match one control unit to one treatment unit (1:1 matching)
    replace = FALSE           # Control observations are not replaced

)

# Extract matched data
matched_data <- match.data(match_model)
```

```{r}
summary(match_model)
```

**2C.Q1** Using the `summary()` output: Which covariate had the largest and smallest `Std. Mean Diff.` before matching. Next, compare largest/smallest `Std. Mean Diff.` after matching.

*Response:*

Before matching, the standardized mean difference (SMD) for `log_median_inc` was ‚àí0.0636, indicating the smallest imbalance, while `log_pop_denc` had the largest imbalance (‚àí0.8897). After matching, `log_median_inc` remains the smallest with an SMD of 0.0002, and `log_pop_denc` remains the largest (‚àí0.0329). However, after matching, all covariates have absolute SMDs well below 0.1, showing they are well balanced.

------------------------------------------------------------------------

#### 2D. Create a "love plot" using `love.plot()` ‚ù§Ô∏è

üìú [Documentation - cobalt](https://ngreifer.github.io/cobalt/)

-   Plot mean differences for data before & after matching across all pre-treatment covariates
-   This is an effective way to evaluate how effective matching was at achieving balance.

------------------------------------------------------------------------

-   Make a love plot of standardized mean differences (SMDs) before vs after matching.
-   Include a threshold line at 0.1.
-   In love plot display `mean.diffs`

```{r}
new_names <- data.frame(
    old = c("log_home_val_07", "p_uni_degree", "log_median_inc", "log_pop_denc"),
    new = c("Home Value (log)", "Percent University Degree",
            "Median Income (log)", "Population Density (log)"))

# Love plot
love.plot(match_model, stats = "mean.diffs",
          thresholds = c(m = 0.1),
          var.names = new_names)
```

**2D.Q1** Interpret the love plot in your own words:

*Response:*

Before matching, several covariates show large imbalances between the treated and control groups. After matching, all standardized mean differences are close to zero and below the 0.1 threshold, indicating strong improvement in balance.

------------------------------------------------------------------------

### Propensity score matching

------------------------------------------------------------------------

#### 2E. Propensity Score Matching (PSM)

-   Estimate 1:1 nearest-neighbor Propensity Score Matching
-   Same code as above except change `distance = "logit"`

```{r}

set.seed(2412026)

propensity_scores <- matchit(
    
    proposed_turbine_3km ~ log_home_val_07 + p_uni_degree + 
        log_median_inc + log_pop_denc,
    data = match_data, 
    method = "nearest",       # Nearest neighbor matching
    distance = "logit",       # Logit distance
    ratio = 1,                # Match one control unit to one treatment unit (1:1 matching)
    replace = FALSE           # Control observations are not replaced

)

# Extract matched data
propensity_scores_data <- match.data(propensity_scores)

# Check the summary
summary(propensity_scores)

love.plot(propensity_scores, stats = "mean.diffs",
          thresholds = c(m = 0.1),
          var.names = new_names)
```

------------------------------------------------------------------------

#### Create table displaying covariate balance using `cobalt::bal.tab()`

üìú [Documentation - cobalt](https://ngreifer.github.io/cobalt/)

Use `bal.tab()` to report balance before and after matching.

```{r}
propensity <- data.frame(
    old = c("log_home_val_07", "p_uni_degree", "log_median_inc", "log_pop_denc"),
    new = c("Home Value (log)", "Percent University Degree",
            "Median Income (log)", "Population Density (log)"))

bal.tab(propensity_scores, 
        var.names = propensity)

bal.tab(match_model, 
        var.names = new_names)
```

**2E.Q1** Compare Mahalanobis vs propensity score matching. Which method did a better job at achieving balance?

*Response:*

Both methods substantially improve balance, as all post-matching standardized mean differences are below 0.1. However, Mahalanobis matching achieves a slightly closer balance to zero across covariates (for example, the largest SMD is 0.033 under Mahalanobis versus 0.046 under propensity score matching). This indicates slightly better covariate balance under the Mahalanobis distance, though both methods perform well.

------------------------------------------------------------------------

#### 2F. Estimate an effect in the matched sample

Using the matched data (Mahalanobis method), estimate the effect of treatment on the change in incumbent vote share (`change_liberal`).

```{r}

reg_match <- lm(change_liberal ~ proposed_turbine_3km, 
                data = match_data)

summ(reg_match, model.fit = FALSE)
```

**2F.Q1** Have you identified a causal estimate using this approach: Why or why not?

*Response:*

Yes, we are able to indicate that there are causal estimate (estimates of ‚àí0.06 with p \< 0.01); however, statistical significance alone does not imply causality. It depends on whether matching successfully removed confounding. While matching reduced all standardized mean differences from as large as 0.89 before matching to below 0.1 after matching, this only demonstrates balance on observed effect of proposed turbine, and does not rule out bias from unobserved confounders.

**2F.Q2** When using a matching method, what is the main threat to causal identification?

*Response:*

The main threat to causal identification in matching is unobserved confounding variables that affect both turbine placement and voting behavior (but are not accounted for in the matching process). Although matching reduced imbalance on observed covariates (all post-matching standardized mean differences are below 0.1), this does not guarantee that treatment assignment is random. The authors explicitly note that selection bias in project location is a central challenge for causal inference, motivating the use of fixed effects and instrumental variables as robustness checks (Stokes 2015, p. 3).

**2F.Q3** Describe why the treatment estimate represents the `Average Treatment for the Treated (ATT)` and explain why this is the case relative to estimation of the `Average Treatment Effect (ATE)`.

*Response:*

Only looking at the effects of the controls on the treatment groups in our subset.

`Average Treatment for the Treated (ATT)`: ATT measures the average effect for precincts that actually received treatment. Matching constructs a control group that closely resembles the treated units, so the estimate reflects the effect on treated precincts.

`Average Treatment Effect (ATE)`: ATE would represent the effect for all precincts, which cannot generally be identified without additional assumptions.

------------------------------------------------------------------------

### Part 3: Panel Data, Fixed Effects, and Difference-in-Difference

**Data source:** [Dataverse-Stokes2015](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SDUGCC)

------------------------------------------------------------------------

#### **3A:** Read in the panel data + code variables `precinct_id` and `year` as factors

```{r}

panel_data <- read_csv(here("data", "Stokes15_panel_data.csv")) %>% 
    mutate(precinct_id = as.factor(precinct_id),
           year = as.factor(year))

# HINT: Try running `tabyl(panel_data$year)`. Review article to make sense of the row numbers (n).
tabyl(panel_data$year)
```

**3A.Q1:** Why are there 18,558 rows in `panel_data`?

*Response:*

There is 6186 rows for each years `2003`, `2007`, and `2011` which adds up to 18,558 rows.

```{r}
# How many years are included in the panel?
length(unique(panel_data$year))

# How many precincts are there?
panel_data %>% 
    group_by(year) %>% 
    summarise(num_precinct = n())
```

**3A.Q2:** How many unique precincts are *ever treated* (i.e., `proposed` & `operational`)?

*Response:*

There are 184 precincts that ever experienced a turbine proposal and 52 that have an operational turbine, adding together to 236 unique precincts are ever treated.

```{r}
panel_data %>%
  group_by(precinct_id) %>%
  summarise(
    ever_proposed    = any(proposed_turbine == 1, na.rm = TRUE),
    ever_operational = any(operational_turbine == 1, na.rm = TRUE),
    .groups = "drop") %>%
  summarise(
    n_ever_proposed    = sum(ever_proposed),
    n_ever_operational = sum(ever_operational))
```

------------------------------------------------------------------------

#### **3B.** Plot and evaluate parallel trends: Replicate `Figure.2` (Stokes, 2015)

1.  Create indicators for whether each precinct is ever treated by 2011 (`treat_p`, `treat_o`; separate indicator for proposals and operational turbines).
2.  Plot mean incumbent vote share by year for treated vs control precincts (with 95% CIs).
3.  Facet by turbine type (proposed & operational)

Step 1: Prepare data

```{r}
trends_data <- panel_data %>%
  group_by(precinct_id) %>%
  mutate(
    treat_p = as.integer(any(proposed_turbine == 1, na.rm = TRUE)),  # ever proposed (in any year)
    treat_o = as.integer(any(operational_turbine == 1, na.rm = TRUE))) %>% # ever operational (in any year)
  ungroup() %>% 
  pivot_longer(c(treat_p, treat_o),
               names_to = "turbine_type", values_to = "treat") %>% 
  mutate(
      turbine_type = factor(turbine_type,
                            levels = c("treat_p", "treat_o"),
                            labels = c("Proposed turbines", "Operational turbines")),  
    status = if_else(treat == 1, "Treated", "Control"),
    year   = factor(year))
```

Step 2: Create trends plot

```{r}
pd <- position_dodge(width = 0.15)

trends_data %>%
  group_by(turbine_type, status, year) %>%
  summarise(
    mean = mean(perc_lib, na.rm = TRUE),
    n    = sum(!is.na(perc_lib)),
    se   = sd(perc_lib, na.rm = TRUE) / sqrt(n), 
    ci   = qt(.975, df = pmax(n - 1, 1)) * se,
    .groups = "drop") %>%
ggplot(aes(year, mean, color = status, group = status)) +
  geom_line(position = pd, linewidth = 1.2) +
  geom_point(position = pd, size = 2.6) +
  geom_errorbar(
    aes(ymin = mean - ci, ymax = mean + ci),
    position = pd, width = .12, linewidth = .7, color = "black") +
  facet_wrap(~ turbine_type, nrow = 1) +
  scale_color_manual(values = c(Control = "#0072B2", Treated = "#B22222")) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  coord_cartesian(ylim = c(.20, .57)) +
  labs(
    title = "Figure 2. Trends in the Governing Party‚Äôs Vote Share",
    x = "Election Year",
    y = "Liberal Party Vote Share",
    color = NULL) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "bottom",
    strip.text = element_text(face = "bold"))
```

**3B.Q1:** Write a short paragraph assessing the parallel trends assumption for each outcome.

*Response (4-6 sentences):*

For the proposed turbines, the treated and control precincts show similar trends in Liberal vote share before treatment, between 2003 and 2007, which supports the parallel-trends assumption. After 2007, treated precincts experienced a larger decline in controls, consistent with an electoral backlash following turbine proposals. For operational turbines, the treated group is smaller, making it harder to assess pre-treatment trends. However, the post-treatment decline is larger, suggesting a stronger effect once turbines become operational. Overall, parallel trends appear more plausible for the proposed turbines than for the operational turbines, though both show clear divergence after treatment. This pattern is consistent with the trends shown in Figure 2 of Stokes (2015, p. 9).

------------------------------------------------------------------------

### Estimating Fixed Effects Models (DiD) for proposals

$$
Y_{it} = \alpha_0 + \beta \cdot \text{proposed\_turbine}_{it} + \gamma_i + \delta_t + \varepsilon_{it}
$$

-   $Y_{it}$ is the vote share for the Liberal Party in precinct *i* in time *t*
-   $\beta$ is the treatment effect of a turbine being proposed within a precinct
-   $\gamma_i$ is the precinct fixed effect
-   $\delta_t$ is the year fixed effect

------------------------------------------------------------------------

### Example 1: Randomly sample 40 precincts

-   To illustrate the "dummy variable method" of estimating fixed effects using the the general `lm()` function we are going to randomly sample 40 precincts (20 "treated" precincts with proposed turbines).
-   If we attempted to use this approach with the full sample estimating all 6185 (n-1) precinct-level coefficients is impractical (it would take a long time).

```{r}
set.seed(40002026)

precinct_frame <- panel_data %>%
  group_by(precinct_id) %>%
  summarise(proposed_turbine_any = as.integer(any(proposed_turbine == 1, 
                                                  na.rm = TRUE)),
            .groups = "drop")

ids_40 <- precinct_frame %>%
  group_by(proposed_turbine_any) %>%
  slice_sample(n = 20) %>%
  ungroup() %>%
  select(precinct_id)

sample_40_precincts <- panel_data %>%
  semi_join(ids_40, by = "precinct_id")
```

------------------------------------------------------------------------

#### **3C:** Estimate a fixed effects model using `lm()` with fixed effects added for `precinct` and `year` using the sample of 40 precincts just created.

```{r}
model1_ff <- lm(perc_lib ~ proposed_turbine + 
                    precinct_id + year + operational_turbine,
                sample_40_precincts) 
    
summ(model1_ff, model.fit = FALSE)
```

```{r}
summ(model1_ff, model.fit = FALSE, digits = 3, robust = TRUE)
```

**3C.Q1:** Intuition check: Is the *signal-to-noise* ratio for the treatment estimate greater than *2-to-1*?

*Response:*

No, because the coefficient estimate (0.042) is not at least twice its standard error (0.041), so the signal-to-noise ratio is below 2.

> HINT: Add the argument `digits = 3` to the `summ()` function above

**3C.Q2:** Re-run the `summ()` function using the *heteroscedasticity robust standard error adjustment* (`robust = TRUE`). Did the standard error (S.E.) estimates change? Explain why.

*Response:*

Yes, the standard errors (SE) changed. Robust standard errors adjust for heteroskedasticity, or unequal error variances. In panel settings, the usual OLS SEs can be too optimistic, so robust SEs are often larger and more reliable. The SE changed when the SE estimates from the original method increased. It is possible because the sampled data were assumed to have normally distributed residuals; however, repeated measurements within places can introduce correlated errors.

**3C.Q3:** Compare results of the model above to the findings from the fixed effects analysis in the Stokes (2015) study. Why might the results be similar or different?

*Response:*

The results from the paper and from our sample can differ because this model uses only a small random sample of 40 precincts, which makes the estimate less precise and noisier. Also, the ‚Äúdummy variable‚Äù fixed effects approach with many indicators can behave differently in small samples. In the full dataset, the estimate should stabilize, less standard errors, and closely resemble the study‚Äôs main fixed-effects results.

**3C.Q4:** In your own words, explain why it is advantageous from a causal inference perspective to include year and precinct fixed effects. Explain how between-level and within-level variance is relevant to the problem of omitted variable bias (OVB).

*Response (2-4 sentences):*

Precinct fixed effects control for time-invariant differences across precincts (long-run political preference, geographical location, or how rural), which could otherwise confound the relationship between turbines and liberal vote share. Year fixed effects control for province-wide shocks in a given election year that affect all precincts similarly. Together, these fixed effects reduce omitted-variable bias by focusing on within-precinct changes over time rather than on comparisons across precincts.

------------------------------------------------------------------------

#### **3D.** Now using the full sample, estimate the treatment effect of wind turbine proposals on incumbent vote share. Use `feols()` from the `{fixest}` package to estimate the fixed effects.

See vignette here: [fixest walkthrough](https://cran.r-project.org/web/packages/fixest/vignettes/fixest_walkthrough.html#11_Estimation)

```{r}

model2_ff <- feols(perc_lib ~ proposed_turbine | 
                       precinct_id + year,
                   data = panel_data,
                   cluster = ~precinct_id)

summary(model2_ff)
```

**3D.Q1:** Interpret the model results and translate findings to be clear to an audience that may not have a background in causal inference (Econometrics) methods.

In panel data settings, why is clustering by precinct important (i.e., `cluster = ~precinct_id`) ?‚Äù

*Response (4-6 sentences):*

The estimate of about ‚àí0.04157 means that when a precinct has a turbine proposal, the Liberal Party‚Äôs vote share is about \~4.2 percentage points lower on average, compared to other years in that same precinct, after accounting for precinct and year fixed effects. This suggests an electoral backlash associated with turbine proposals. *Clustering by precinct* is important because each precinct appears multiple times across years, so the errors are correlated within a precinct. If we ignore that correlation, SEs can be too small, leading us to overstate our confidence in the effect.

------------------------------------------------------------------------

#### **3E.** Estimate the treatment effect of *operational wind turbines* on incumbent vote share. Use the same approach as the previous model.

```{r}
model3_ff <- feols(perc_lib ~ operational_turbine | 
                       precinct_id + year,
                   data = panel_data,
                   cluster = ~ precinct_id)

summary(model3_ff)
```

**3E.Q1:** Interpret the `model3_ff` results as clearly and **concisely** as you can.

*Response:*

The estimate of about ‚àí0.0928 means that when turbines are operational in a precinct, the Liberal Party‚Äôs vote share is about \~9.3 percentage points lower, compared to other years in that same precinct, controlling for precinct and year fixed effects. This indicates a larger electoral backlash against the incumbent Liberal government once turbines are actually built and running.

**3E.Q2:** Why do you think the effect of proposed wind turbines is different from operational wind turbines. Develop your own theory about why incumbent vote share is affected in this way. Use the Stokes (2015) study to inform your response as needed.

*Response:*

Operational turbines create more visible local impacts, as people can see and hear them and associate them with changes in their community, so more voters notice. However, proposals can also trigger opposition. For example, some residents may think the project might not happen and be skeptical. Once turbines are operational, the local costs feel more real and persistent, so the electoral punishment of incumbents can be stronger. This interpretation aligns with the authors‚Äô discussion that nearby communities perceive negative effects from wind projects once they are present, contributing to detectable vote share declines. (Stokes 2015, p. 12).

------------------------------------------------------------------------

```{r, message = TRUE, echo = FALSE, eval = FALSE}

library(praise); library(cowsay)

praise("${EXCLAMATION}! üöÄ Great work - You are ${adjective}! üí´")

say("The End", "duck")
```
